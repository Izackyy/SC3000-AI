{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Balancing a Pole on a Cart**\n",
    "### 24S2 SC3000/CZ3005 Assignment 1\n",
    "---\n",
    "### Team Members\n",
    "- Toh Jun Sheng\n",
    "- Isaac Wong Jia Kai\n",
    "- Tio Sher-min\n",
    "---\n",
    "### Contribution\n",
    "\n",
    "#### Training the RL Agent\n",
    "\n",
    "#### Task 1: Development of an RL Agent\n",
    "\n",
    "\n",
    "#### Task 2: Demonstrate the effectiveness of the RL agent\n",
    "\n",
    "#### Task 3: Render one episode played by the developed RL agent\n",
    "\n",
    "#### Task 4: Format the Jupyter notebook\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym pyvirtualdisplay \n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!pip install pygame gymnasium[classic-control] pyvirtualdisplay\n",
    "!pip install --upgrade setuptools \n",
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dependencies and define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import RecordVideo\n",
    "gymlogger.set_level(40) #error only\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: Loading CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the action and observation space of this environment. Discrete(2) means that there are two valid discrete actions: 0 & 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space is given below. The first two arrays define the min and max values of the 4 observed values, corresponding to cart position, velocity and pole angle, angular velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call each round of the pole-balancing game an \"episode\". At the start of each episode, make sure the environment is reset, which chooses a random initial state, e.g., pole slightly tilted to the right. This initialization can be achieved by the code below, which returns the observation of the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "print(\"Initial observations:\", observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CartPole environment, there are two possible actions: 0 for pushing to the left and 1 for pushing to the right. For example, we can push the cart to the left using code below, which returns the new observation, the current reward, an indicator of whether the game ends, and some additional information (not used in this project). For CartPole, the game ends when the pole is significantly tilted or you manage to balance the pole for 500 steps. You get exactly 1 reward for each step before the game ends (i.e., max cumulative reward is 500)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, terminated, truncated, info = env.step(0)\n",
    "done = terminated or truncated\n",
    "\n",
    "print(\"New observations after choosing action 0:\", observation)\n",
    "print(\"Reward for this step:\", reward)\n",
    "print(\"Is this round done?\", done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can play a full round of the game using a naive strategy (always choosing action 0), and show the cumulative reward in the round. Note that reward returned by env.step(*) corresponds to the reward for current step. So we have to accumulate the reward for each step. Clearly, the naive strategy performs poorly by surviving only a dozen of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, _ = env.reset()\n",
    "cumulative_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    observation, reward, terminated, truncated, info = env.step(0)\n",
    "    done = terminated or truncated\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(\"Cumulative reward for this round:\", cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: Q-Learning Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Preparations before training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Q-learning works with finite (discrete) state spaces, but CartPole’s state is continuous, meaning there are infinite possible states. <br>\n",
    "The agent can’t store Q-values for infinite states, so we must discretize the state space into bins. </h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize the continuous observation space into bins\n",
    "state_bins = [\n",
    "    np.linspace(-4.8, 4.8, 25),     # Cart Position\n",
    "    np.linspace(-5, 5, 25),         # Cart Velocity\n",
    "    np.linspace(-0.418, 0.418, 25), # Pole Angle\n",
    "    np.linspace(-5, 5, 25)          # Pole Angular Velocity\n",
    "]\n",
    "\n",
    "# Function to discretize a continuous state\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = []\n",
    "    for s, bins in zip(state, state_bins):\n",
    "        index = np.digitize(s, bins)\n",
    "        index = min(max(index, 0), len(bins))  # Clip to valid range\n",
    "        discrete_state.append(index)\n",
    "    return tuple(discrete_state)\n",
    "\n",
    "# Q-table initialization \n",
    "q_table = np.zeros([len(b) + 1 for b in state_bins] + [env.action_space.n])  # All entries initialized as 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> The observations for the current continuous state will be saved into the respective bins indicated in the discrete state tuple. These bins shall make up the q-table, a multi-dimensional table storing the action-value estimates for each discrete state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check initial state\n",
    "observation, _ = env.reset()\n",
    "discrete_obs = get_discrete_state(observation)\n",
    "print(\"Continuous state:\", observation)\n",
    "print(\"Discrete state:\", tuple(int(x) for x in discrete_obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Next, we define the hyperparameters that will be used by the Q-Learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1            # Learning rate (Alpha)\n",
    "gamma = 0.99           # Discount factor (Gamma)\n",
    "epsilon = 1.0          # Initial exploration rate\n",
    "epsilon_min = 0.05\n",
    "episodes = 10000       # Training episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> The functionalities of the agent are as such: \n",
    "\n",
    "- For each step during an episode, a random number between 0 and 1 will be generated and compared with the current epsilon value\n",
    "    - if the value is smaller than epsilon, a random action shall be taken. Else a greedy action will be taken based on the q_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-greedy action selection\n",
    "def epsilon_greedy_action(state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return env.action_space.sample()  # Explore\n",
    "    else:\n",
    "        return np.argmax(q_table[state])  # Exploit\n",
    "\n",
    "\n",
    "def q_policy_agent(observation):\n",
    "    discrete_state = get_discrete_state(observation)\n",
    "    return np.argmax(q_table[discrete_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Now we shall proceed to training the agent with 10,000 episodes in order to fill the q_table </h4> \n",
    "<h5> Note: This will take approximately 90 secs to run </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "rewards = []\n",
    "total_mean_reward = 0\n",
    "epsilon_decay = (epsilon-epsilon_min)/(episodes*0.8)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = get_discrete_state(env.reset()[0])\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = epsilon_greedy_action(state, epsilon)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state = get_discrete_state(next_obs)\n",
    "\n",
    "        # Q-learning update rule\n",
    "        best_next_action = np.argmax(q_table[next_state])\n",
    "        td_target = reward + gamma * q_table[next_state][best_next_action]\n",
    "        td_error = td_target - q_table[state][action]\n",
    "        q_table[state][action] += alpha * td_error\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "    total_mean_reward += total_reward\n",
    "\n",
    "    # Epsilon decay\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon -= epsilon_decay\n",
    "\n",
    "    # Print progress\n",
    "    if episode % 500 == 0 and episode != 0:\n",
    "        print(f\"Episode {episode}, Reward: {total_mean_reward/500}, Epsilon: {epsilon:.3f}\")\n",
    "        total_mean_reward = 0\n",
    "\n",
    "# Plotting cumulative reward\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Q-learning Agent Training Performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> By allowing epsilon to decay linearly such that the min value is reached after around 8000 episodes, we can observe the epsilon-greedy process at work, slowly shifting from more exploration to more exploitation. <br>\n",
    "Since epsilon starts at 1 and gradually decreases over time, in early episodes, the agent explores more (choosing a random action frequently). As epsilon gets smaller, the agent exploits more (choosing the best-known action more often).\n",
    "\n",
    "- Epsilon only decreases if the agent's reward is improving, encouraging exploration when needed. \n",
    "- It stops decreasing at 0.05, ensuring the agent never stops exploring completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> <b> Task 1 </b> </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task 1 Code ===\n",
    "observation = env.reset()[0]\n",
    "discrete_obs = get_discrete_state(observation) \n",
    "action = q_policy_agent(observation)\n",
    "\n",
    "print(\"Continuous state:\", observation)\n",
    "print(\"Discrete state:\", tuple(int(x) for x in discrete_obs)) \n",
    "print(\"Chosen action:\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> As we can see, in this particular observation state, the best action found by the agent after training is 0 (left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task 2: Evaluation of Trained Agent ===\n",
    "\n",
    "epsilon = 0.0\n",
    "episode_results = []\n",
    "best_episode_index = -1\n",
    "best_reward = 0\n",
    "\n",
    "# Use unwrapped environment for evaluation (faster)\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "for episode in range(100):\n",
    "    obs = env.reset(seed=episode)[0]  # fixed seed = reproducible episode\n",
    "    state = get_discrete_state(obs)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = epsilon_greedy_action(state, epsilon)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        state = get_discrete_state(next_obs)\n",
    "        total_reward += reward\n",
    "\n",
    "    episode_results.append(total_reward)\n",
    "\n",
    "    if total_reward > best_reward:\n",
    "        best_reward = total_reward\n",
    "        best_episode_index = episode\n",
    "\n",
    "env.close()\n",
    "\n",
    "# === Plot rewards ===\n",
    "plt.plot(episode_results)\n",
    "plt.title('Cumulative reward for each episode')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()\n",
    "\n",
    "# === Print result ===\n",
    "print(\"Average cumulative reward:\", np.mean(episode_results))\n",
    "print(\"Is my agent good enough?\", np.mean(episode_results) > 195)\n",
    "print(\"Best episode index:\", best_episode_index)\n",
    "print(\"Best reward:\", best_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task 3: Record and Replay the Best Episode ===\n",
    "\n",
    "# Create a new environment that records the best episode\n",
    "env = RecordVideo(\n",
    "    gym.make(\"CartPole-v1\", render_mode=\"rgb_array\"),\n",
    "    video_folder=\"video\",\n",
    "    episode_trigger=lambda e: True\n",
    ")\n",
    "\n",
    "# Reset with the same seed used in Task 2\n",
    "observation = env.reset(seed=best_episode_index)[0]\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = q_policy_agent(observation)\n",
    "    observation, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "env.close()\n",
    "\n",
    "# === Display recorded video ===\n",
    "show_video()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check initial state\n",
    "observation, _ = env.reset()\n",
    "discrete_obs = get_discrete_state(observation)\n",
    "print(\"Continuous state:\", observation)\n",
    "print(\"Discrete state:\", discrete_obs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
